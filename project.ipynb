{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 1985\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing random policy:\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION 0 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION 0 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION 0 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION -1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION -1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION -1 NEXT REWARD 0 NEXT STATE 2040\n",
            "CURR STATE 2040 ACTION 1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION 1 NEXT REWARD 0 NEXT STATE 2043\n",
            "CURR STATE 2043 ACTION -1 NEXT REWARD 0 NEXT STATE 1914\n",
            "CURR STATE 1914 ACTION -1 NEXT REWARD 0 NEXT STATE 1913\n",
            "CURR STATE 1913 ACTION -1 NEXT REWARD 0 NEXT STATE 2040\n",
            "CURR STATE 2040 ACTION 1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION -1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION -1 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 0 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 0 NEXT REWARD 0 NEXT STATE 2041\n",
            "CURR STATE 2041 ACTION 1 NEXT REWARD 0 NEXT STATE 2042\n",
            "CURR STATE 2042 ACTION 1 NEXT REWARD 0 NEXT STATE 2043\n",
            "CURR STATE 2043 ACTION 1 NEXT REWARD 0 NEXT STATE 1916\n",
            "CURR STATE 1916 ACTION 1 NEXT REWARD 0 NEXT STATE 1789\n",
            "CURR STATE 1789 ACTION 0 NEXT REWARD 0 NEXT STATE 1653\n",
            "CURR STATE 1653 ACTION 1 NEXT REWARD 0 NEXT STATE 1518\n",
            "CURR STATE 1518 ACTION 0 NEXT REWARD 0 NEXT STATE 1510\n",
            "CURR STATE 1510 ACTION 1 NEXT REWARD 0 NEXT STATE 1503\n",
            "CURR STATE 1503 ACTION 0 NEXT REWARD 0 NEXT STATE 1623\n",
            "CURR STATE 1623 ACTION 1 NEXT REWARD 0 NEXT STATE 1736\n",
            "CURR STATE 1736 ACTION -1 NEXT REWARD 0 NEXT STATE 1871\n",
            "\n",
            "Results saved to log-random.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import csv\n",
        "\n",
        "#print the output\n",
        "print(\"Testing random policy:\")\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "\n",
        "# Now save to TSV file\n",
        "with open('log-random.tsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    # Write header\n",
        "    writer.writerow(['curr_state', 'curr_action', 'next_reward', 'next_state'])\n",
        "    \n",
        "    # Write data\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        writer.writerow([curr_state, curr_action, next_reward, next_state])\n",
        "\n",
        "print(\"\\nResults saved to log-random.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total state-action pairs learned: 5314\n",
            "Maximum Q-value: 1.0000\n",
            "Minimum Q-value: 0.0000\n",
            "Average Q-value: 0.0202\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Initialize Q-table as a dictionary\n",
        "# Key: (state, action), Value: Q-value\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "        #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "        # pass\n",
        "        # Get current Q-value (initialize to 0 if not seen before)\n",
        "        old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "        \n",
        "        # Get max Q-value for next state over all possible actions\n",
        "        next_actions = simulator.get_actions(next_state)\n",
        "        max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "        \n",
        "        # Q-Learning update: Q(s,a) = Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
        "        # With alpha=1, this simplifies to: Q(s,a) = r + gamma * max_a' Q(s',a')\n",
        "        new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "        \n",
        "        # Update Q-table\n",
        "        Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "# Output results\n",
        "print(f\"Total state-action pairs learned: {len(Q)}\")\n",
        "print(f\"Maximum Q-value: {max(Q.values()):.4f}\")\n",
        "print(f\"Minimum Q-value: {min(Q.values()):.4f}\")\n",
        "print(f\"Average Q-value: {sum(Q.values())/len(Q):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W8cFRd7uPGqy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to q-random.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Open file to save results\n",
        "with open('q-random.tsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    # Write header\n",
        "    writer.writerow(['curr_state', 'curr_action', 'next_reward', 'next_state', 'old_value', 'new_value'])\n",
        "    \n",
        "    # Re-initialize Q-table for this logging run\n",
        "    Q_log = {}\n",
        "    \n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "            # Get current Q-value (initialize to 0 if not seen before)\n",
        "            old_value = Q_log.get((curr_state, curr_action), 0.0)\n",
        "            \n",
        "            # Get max Q-value for next state over all possible actions\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_q = max([Q_log.get((next_state, a), 0.0) for a in next_actions])\n",
        "            \n",
        "            # Q-Learning update\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "            \n",
        "            # Update Q-table\n",
        "            Q_log[(curr_state, curr_action)] = new_value\n",
        "            \n",
        "            # Write to file\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "\n",
        "print(\"Results saved to q-random.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    epsilon = 0.25 # hard-coded value\n",
        "    if random.random() < epsilon:\n",
        "        # Explore: choose random action\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        # Exploit: choose action with highest Q-value\n",
        "        q_values = [(action, Q.get((state, action), 0.0)) for action in actions]\n",
        "        # Find the action with maximum Q-value\n",
        "        best_action = max(q_values, key=lambda x: x[1])[0]\n",
        "        return best_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total state-action pairs learned: 3410\n",
            "Maximum Q-value: 1.0000\n",
            "Minimum Q-value: 0.0000\n",
            "Average Q-value: 0.0075\n",
            "\n",
            "Results saved to q-greedy.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Reset Q-learning state (reinitialize Q-table)\n",
        "Q = {}\n",
        "alpha = 1.0  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "\n",
        "with open('q-greedy.tsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    # Write header\n",
        "    writer.writerow(['curr_state', 'curr_action', 'next_reward', 'next_state', 'old_value', 'new_value'])\n",
        "    \n",
        "    for episode in range(32):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "            #print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "            \n",
        "            # Get current Q-value (initialize to 0 if not seen before)\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "            \n",
        "            # Get max Q-value for next state over all possible actions\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "            \n",
        "            # Q-Learning update\n",
        "            new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "            \n",
        "            # Update Q-table\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "            \n",
        "            # Write to file\n",
        "            writer.writerow([curr_state, curr_action, next_reward, next_state, old_value, new_value])\n",
        "            \n",
        "            # if next_reward > 0:\n",
        "            #     # moving to terminal state\n",
        "            #     break\n",
        "\n",
        "# Report results\n",
        "print(f\"Total state-action pairs learned: {len(Q)}\")\n",
        "print(f\"Maximum Q-value: {max(Q.values()):.4f}\")\n",
        "print(f\"Minimum Q-value: {min(Q.values()):.4f}\")\n",
        "print(f\"Average Q-value: {sum(Q.values())/len(Q):.4f}\")\n",
        "print(\"\\nResults saved to q-greedy.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy extracted for 1717 states\n",
            "Policy saved to policy-greedy.tsv\n",
            "\n",
            "Sample policy entries (first 10):\n",
            "  State 3: Action -1\n",
            "  State 4: Action -1\n",
            "  State 5: Action -1\n",
            "  State 6: Action -1\n",
            "  State 7: Action -1\n",
            "  State 10: Action -1\n",
            "  State 11: Action -1\n",
            "  State 13: Action -1\n",
            "  State 17: Action -1\n",
            "  State 18: Action -1\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Extract policy: for each state -> choose the action with the highest Q-value\n",
        "policy = {}\n",
        "\n",
        "# Get all unique states from the Q-table\n",
        "states = set(state for (state, action) in Q.keys())\n",
        "\n",
        "for state in states:\n",
        "    # Get all actions available for this state\n",
        "    actions = simulator.get_actions(state)\n",
        "    \n",
        "    # Find the action with the highest Q-value for this state\n",
        "    best_action = None\n",
        "    best_q_value = float('-inf')\n",
        "    \n",
        "    for action in actions:\n",
        "        q_value = Q.get((state, action), 0.0)\n",
        "        if q_value > best_q_value:\n",
        "            best_q_value = q_value\n",
        "            best_action = action\n",
        "    \n",
        "    policy[state] = best_action\n",
        "\n",
        "# Save policy to file\n",
        "with open('policy-greedy.tsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    # Write column header\n",
        "    writer.writerow(['state', 'action'])\n",
        "    \n",
        "    # Write policy (sorted by state for readability)\n",
        "    for state in sorted(policy.keys()):\n",
        "        writer.writerow([state, policy[state]])\n",
        "\n",
        "# Output results\n",
        "print(f\"Policy extracted for {len(policy)} states\")\n",
        "print(\"Policy saved to policy-greedy.tsv\")\n",
        "\n",
        "# Show some example policy entries\n",
        "print(\"\\nSample policy entries (first 10):\")\n",
        "for i, (state, action) in enumerate(sorted(policy.items())[:10]):\n",
        "    print(f\"  State {state}: Action {action}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training optimal policy:\n",
            "Episode 200/1000 complete. Episodes with reward: 0\n",
            "Episode 400/1000 complete. Episodes with reward: 0\n",
            "Episode 600/1000 complete. Episodes with reward: 0\n",
            "Episode 800/1000 complete. Episodes with reward: 0\n",
            "Episode 1000/1000 complete. Episodes with reward: 0\n",
            "Total state-action pairs learned: 5328\n",
            "Episodes that reached goal: 0/1000\n",
            "Maximum Q-value: 1.0000\n",
            "Minimum Q-value: 0.0000\n",
            "Average Q-value: 0.2392\n",
            "\n",
            "Verifying policy closure:\n",
            "Policy is closed. All reachable states are covered\n",
            "\n",
            "Initial states covered: 71/71\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Reset Q-learning state (reinitialize Q-table)\n",
        "Q = {}\n",
        "alpha = 1.0  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "\n",
        "# Use epsilon-greedy with decreasing epsilon for better convergence\n",
        "def adaptive_epsilon_greedy_policy(state, actions, episode, total_episodes):\n",
        "    # Start with higher exploration, decrease over time\n",
        "    epsilon_start = 0.5\n",
        "    epsilon_end = 0.1\n",
        "    epsilon = epsilon_start - (epsilon_start - epsilon_end) * (episode / total_episodes)\n",
        "    \n",
        "    if random.random() < epsilon:\n",
        "        # Explore: choose random action\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        # Exploit: choose action with highest Q-value\n",
        "        q_values = [(action, Q.get((state, action), 0.0)) for action in actions]\n",
        "        best_action = max(q_values, key=lambda x: x[1])[0]\n",
        "        return best_action\n",
        "\n",
        "# Run many more episodes to get better coverage and convergence\n",
        "num_episodes = 1000\n",
        "max_steps = 2048\n",
        "\n",
        "print(\"Training optimal policy:\")\n",
        "episodes_with_reward = 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(\n",
        "        lambda s, a: adaptive_epsilon_greedy_policy(s, a, episode, num_episodes), \n",
        "        max_steps = max_steps\n",
        "    ):\n",
        "        # Get current Q-value\n",
        "        old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "        \n",
        "        # Get max Q-value for next state\n",
        "        next_actions = simulator.get_actions(next_state)\n",
        "        max_next_q = max([Q.get((next_state, a), 0.0) for a in next_actions])\n",
        "        \n",
        "        # Q-Learning update\n",
        "        new_value = old_value + alpha * (next_reward + gamma * max_next_q - old_value)\n",
        "        \n",
        "        # Update Q-table\n",
        "        Q[(curr_state, curr_action)] = new_value\n",
        "        \n",
        "        # if next_reward > 0:\n",
        "        #     episodes_with_reward += 1\n",
        "        #     # Got reward, end episode\n",
        "        #     break\n",
        "    \n",
        "    # Print progress every 200 episodes\n",
        "    if (episode + 1) % 200 == 0:\n",
        "        print(f\"Episode {episode + 1}/{num_episodes} complete. Episodes with reward: {episodes_with_reward}\")\n",
        "\n",
        "print(f\"Total state-action pairs learned: {len(Q)}\")\n",
        "print(f\"Episodes that reached goal: {episodes_with_reward}/{num_episodes}\")\n",
        "print(f\"Maximum Q-value: {max(Q.values()):.4f}\")\n",
        "print(f\"Minimum Q-value: {min(Q.values()):.4f}\")\n",
        "print(f\"Average Q-value: {sum(Q.values())/len(Q):.4f}\")\n",
        "\n",
        "# Extract optimal policy\n",
        "policy = {}\n",
        "states = set(state for (state, action) in Q.keys())\n",
        "\n",
        "for state in states:\n",
        "    actions = simulator.get_actions(state)\n",
        "    best_action = max(actions, key=lambda a: Q.get((state, a), 0.0))\n",
        "    policy[state] = best_action\n",
        "\n",
        "# Verify closure: ensure all states reachable from initial states are in policy\n",
        "print(\"\\nVerifying policy closure:\")\n",
        "states_to_check = set(simulator.initial_states)\n",
        "visited = set()\n",
        "missing_states = []\n",
        "\n",
        "while states_to_check:\n",
        "    state = states_to_check.pop()\n",
        "    if state in visited:\n",
        "        continue\n",
        "    visited.add(state)\n",
        "    \n",
        "    if state == simulator.terminal_state:\n",
        "        continue\n",
        "    \n",
        "    if state not in policy:\n",
        "        missing_states.append(state)\n",
        "        # Add a default action for missing states\n",
        "        actions = simulator.get_actions(state)\n",
        "        policy[state] = max(actions, key=lambda a: Q.get((state, a), 0.0))\n",
        "    \n",
        "    # Follow the policy to find next state\n",
        "    action = policy[state]\n",
        "    (next_reward, next_state) = simulator.get_next_reward_state(state, action)\n",
        "    \n",
        "    if next_state not in visited:\n",
        "        states_to_check.add(next_state)\n",
        "\n",
        "if missing_states:\n",
        "    print(f\"Added {len(missing_states)} missing states to ensure closure\")\n",
        "else:\n",
        "    print(\"Policy is closed. All reachable states are covered\")\n",
        "\n",
        "# Check coverage of initial states\n",
        "initial_states_covered = sum(1 for init_state in simulator.initial_states if init_state in policy)\n",
        "print(f\"\\nInitial states covered: {initial_states_covered}/{len(simulator.initial_states)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal policy saved to policy-optimal.tsv\n",
            "\n",
            "Average Q-value for initial states: 0.3232\n"
          ]
        }
      ],
      "source": [
        "# Save the optimal policy to file\n",
        "with open('policy-optimal.tsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerow(['state', 'action'])\n",
        "    \n",
        "    for state in sorted(policy.keys()):\n",
        "        writer.writerow([state, policy[state]])\n",
        "\n",
        "print(\"Optimal policy saved to policy-optimal.tsv\")\n",
        "\n",
        "# Calculate average Q-value for initial states\n",
        "initial_state_values = []\n",
        "for init_state in simulator.initial_states:\n",
        "    if init_state in policy:\n",
        "        action = policy[init_state]\n",
        "        value = Q.get((init_state, action), 0.0)\n",
        "        initial_state_values.append(value)\n",
        "    else:\n",
        "        initial_state_values.append(0.0)\n",
        "\n",
        "avg_initial_value = sum(initial_state_values) / len(initial_state_values)\n",
        "print(f\"\\nAverage Q-value for initial states: {avg_initial_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create acknowledgements.txt file\n",
        "acknowledgements_content = \"\"\"Discussed assignment with:\n",
        "No one\n",
        "\n",
        "Libraries used:\n",
        "- random: For implementing random and epsilon-greedy policies\n",
        "- csv: For reading and writing TSV files\n",
        "\n",
        "Additional resources:\n",
        "I used the lecture materials provided in the course, including the project files and lecture videos on Q-learning algorithms.\n",
        "\"\"\"\n",
        "\n",
        "with open('acknowledgements.txt', 'w') as f:\n",
        "    f.write(acknowledgements_content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
